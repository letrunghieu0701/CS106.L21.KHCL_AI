{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Import libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_2048\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hyperparameters***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.1\n",
    "gamma = 0.99\n",
    "memory_size = 5000 # number of experiences being stored in the memory\n",
    "batch_size = 64 # batch size to sample experiences\n",
    "target_update_frequency = 100 # after this amount of actions, update the target-network with the q-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q-Network Class***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            # Hidden layers\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features = 16, out_features = 64), # First hidden layer\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features = 64, out_features = 64), # Second hidden layer\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features = 64, out_features = 64), # Third hidden layer\n",
    "            nn.Tanh(),\n",
    "            # Output layer\n",
    "            nn.Linear(64, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = np.ascontiguousarray(state, dtype=np.float32)\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        q_values = self(state.unsqueeze(0)) # Pytorch require input in terms of batch\n",
    "        best_action_index = torch.argmax(q_values, dim=1)[0]\n",
    "\n",
    "        return best_action_index.detach().item()\n",
    "    \n",
    "    def choose_action_and_print_q_values(self, state):\n",
    "        state = np.ascontiguousarray(state, dtype=np.float32)\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        q_values = self(state.unsqueeze(0)) # Pytorch require input in terms of batch\n",
    "        print(\"q-values: \", q_values)\n",
    "        best_action_index = torch.argmax(q_values, dim=1)[0]\n",
    "        print(f\"Best value: {torch.max(q_values)}, action: {best_action_index.detach().item()}\")\n",
    "\n",
    "        return best_action_index.detach().item()\n",
    "    \n",
    "    def choose_random_action(self, env):\n",
    "        return random.randrange(0, env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create the game and Experience replay memory***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('2048-v0')\n",
    "memory = deque(maxlen = memory_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create the 2 network***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = NeuralNetwork(env)\n",
    "target_net = NeuralNetwork(env)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Train the 2 network***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_episode = 2000\n",
    "epsilon_decay_intervals = num_episode\n",
    "max_num_steps = 10000\n",
    "\n",
    "reward_per_episode = 0.0\n",
    "count_step = 0\n",
    "\n",
    "training_info = {\n",
    "    \"Episode\": np.zeros(shape=num_episode, dtype=np.uint16),\n",
    "    \"Total_reward\": np.zeros(shape=num_episode, dtype=np.uint16),\n",
    "    \"Max_value\": np.zeros(shape=num_episode, dtype=np.uint16),\n",
    "    \"Total_actions\": np.zeros(shape=num_episode, dtype=np.uint32),\n",
    "}\n",
    "\n",
    "start_time = time.process_time()\n",
    "for episode in range(num_episode):\n",
    "    # Reset game\n",
    "    state = env.reset()\n",
    "    reward_per_episode = 0.0\n",
    "    \n",
    "#     epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-epsilon_decay_intervals*episode)\n",
    "    epsilon = np.interp(episode, [0, epsilon_decay_intervals], [max_epsilon, min_epsilon])\n",
    "    \n",
    "    for step in range(max_num_steps):\n",
    "        # Count number of action has been executed to update the target-network\n",
    "        count_step += 1\n",
    "\n",
    "        # Choose action\n",
    "        exploration = np.random.uniform(0, 1)\n",
    "        if exploration <= epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = q_net.choose_action(state)\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        reward_per_episode += reward\n",
    "        \n",
    "        # Get max grid's value\n",
    "        max_grid_value = np.amax(next_state)\n",
    "\n",
    "        # Store experience\n",
    "        experience = (state, action, reward, done, next_state)\n",
    "        memory.append(experience)\n",
    "        \n",
    "        # Change to next state\n",
    "        state = next_state\n",
    "\n",
    "        # Sample batch of experiences to learn\n",
    "        if len(memory) >= batch_size:\n",
    "            # Take batch_size experiences from the memory\n",
    "            experiences = random.sample(memory, batch_size)\n",
    "\n",
    "            states = [ex[0] for ex in experiences]\n",
    "            actions = [ex[1] for ex in experiences]\n",
    "            rewards = [ex[2] for ex in experiences]\n",
    "            dones = [ex[3] for ex in experiences]\n",
    "            next_states = [ex[4] for ex in experiences]\n",
    "\n",
    "            # Change to tensor\n",
    "            states = torch.tensor(states, dtype=torch.float32)\n",
    "            actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(-1) # (batch_size,) --> (batch_size, 1)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(-1)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(-1)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "\n",
    "            # Compute target values using the formulation sample = r + gamma * max q(s', a')\n",
    "            target_q_values = target_net(next_states)\n",
    "            max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0] # index 0 to take the max values, index 1 to take the max values's index\n",
    "            targets = rewards + gamma * (1 - dones) * max_target_q_values\n",
    "\n",
    "            # Compute loss\n",
    "            q_values = q_net(states)\n",
    "            action_q_values = torch.gather(input=q_values, dim =1, index = actions)\n",
    "            loss = nn.functional.mse_loss(action_q_values, targets)\n",
    "\n",
    "            # Gradient descent for q-network\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Update target-netword\n",
    "        if count_step % target_update_frequency == 0:\n",
    "#             print(\"update target-net\")\n",
    "            target_net.load_state_dict(q_net.state_dict())\n",
    "            \n",
    "        if done:\n",
    "            print(f\"Episode {episode+1}   Reward: {reward_per_episode}   Max value: {max_grid_value}   Total actions: {step}\")\n",
    "#             env.render()\n",
    "            \n",
    "            # Store information about an episode\n",
    "            training_info[\"Episode\"][episode] = episode+1\n",
    "            training_info[\"Total_reward\"][episode] = reward_per_episode\n",
    "            training_info[\"Max_value\"][episode] = max_grid_value\n",
    "            training_info[\"Total_actions\"][episode] = step\n",
    "            break\n",
    "\n",
    "end_time = time.process_time()\n",
    "print('\\nTime To train: ', end_time - start_time, \" seconds\")\n",
    "\n",
    "# Saving training_info and the q-net\n",
    "df = pd.DataFrame(training_info)\n",
    "name = \"3layers_1000eps\" # Name of the model and the excel file to store results\n",
    "df.to_excel(excel_writer=\"training_info_\" + name + \".xlsx\", index=False)\n",
    "torch.save(q_net.state_dict(), \"model_\" + name + \".pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Load the model has been trained to play***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(env)\n",
    "model.load_state_dict(torch.load(\"model_3layers_2000eps_5000memorySize.pth\"))\n",
    "play_multiple_times(env, model, num_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
